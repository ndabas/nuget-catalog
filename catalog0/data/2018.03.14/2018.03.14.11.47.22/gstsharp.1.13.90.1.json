{
  "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json",
  "@type": [
    "PackageDetails",
    "catalog:Permalink"
  ],
  "authors": "thiblahute",
  "catalog:commitId": "b476f9fe-22b1-45db-b90e-e95dec4ec650",
  "catalog:commitTimeStamp": "2018-03-14T11:47:22.1063541Z",
  "copyright": "Copyright 2018",
  "created": "2018-03-14T11:38:14.447Z",
  "description": "gstreamer-sharp\n=========\n\ngstreamer-sharp is a .NET/mono binding for Gstreamer\ngenerated from gobject-introspection data using the [bindinator].\ngstreamer-sharp currently wraps the API exposed by Gstreamer 1.12\nand is compatible with newer gstreamer versions. It was developed\nunder GSoC 2014 for the mono organization. gstreamer-sharp covers\nthe core and base gstreamer libraries.\n\nPrerequisites\n----\nThese libraries are needed for clutter-sharp to compile:\n* gstreamer core, base and good 1.14 or higher\n* [gtk-sharp] 3.22.6 or higher - *NOTE: This can be built as a meson subproject if using the meson build system.*\n\nYou will also need various .NET/mono bits (mcs and al). On debian-based distros\nyou can install these with:\n\n    sudo apt-get install mono-mcs mono-devel\n\nBuilding & Installing\n----\n\n    meson build && ninja -C build/\n\nHACKING\n-------\n\nWhile hacking on the code generator or the `.metadata` files, you will\nneed to force code regeneration with `ninja update-code`, a full rebuild\nis triggered right after.\n\nUpdating to new GStreamer version\n--------------------------------\n\n* Make sure [bindinator] is installed on the system\n\nMake sure you are in an environement where latest `.gir` files are available (either install\nor through the `$GI_TYPELIB_PATH` env var).\n\n    ninja -C update-all\n\n* Verify newly generated code and `git add` files in `sources/generated/` and `ges/generated`\n* Commit\n\nSupported Platforms\n----\n* Linux\n* Mac OS X\n\nQuick Start\n----\ngstreamer-sharp provides ports of all samples from gst-docs in the samples folder.\n\nDocumentation\n----\n\nSince this is a gobject-introspection binding the recommended documentation is\nthe native [gstreamer] documentation. A monodoc generated documentation will be installed.\n\nRoadmap\n----\n* Add an easy way to compile on Windows\n* iOS and Android support\n* Provide binaries for these platforms\n\nLicense\n----\ngstreamer-sharp is licensed under the [LGPL 2.1](https://www.gnu.org/licenses/lgpl-2.1.html)\n\n[bindinator]:https://github.com/GLibSharp/bindinator\n[gtk-sharp]:https://github.com/GLibSharp/GtlSharp\n[gstreamer]: http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gstreamer/html/",
  "iconUrl": "https://gstreamer.freedesktop.org/images/header-logo-top.png",
  "id": "GstSharp",
  "isPrerelease": false,
  "lastEdited": "2018-03-14T11:45:22.01Z",
  "licenseUrl": "https://cgit.freedesktop.org/gstreamer/gstreamer-sharp/plain/COPYING",
  "listed": true,
  "packageHash": "qUbPb7NBBmso+gHqaQQFd1ixEwirKlw9WVYCLAtqxnS3dP1P5vBJa1STJg38EUx2HbM89ZoJNPt2w9V95jCsVA==",
  "packageHashAlgorithm": "SHA512",
  "packageSize": 711215,
  "projectUrl": "https://gstreamer.freedesktop.org/modules/gstreamer-sharp.html",
  "published": "2018-03-14T11:38:14.447Z",
  "releaseNotes": "GSTREAMER 1.14 RELEASE NOTES\n\n\nGStreamer 1.14.0 has not been released yet. It is scheduled for release\nin early March 2018.\n\nThere are unstable pre-releases available for testing and development\npurposes. The latest pre-release is version 1.13.91 (rc2) and was\nreleased on 12 March 2018.\n\nSee https://gstreamer.freedesktop.org/releases/1.14/ for the latest\nversion of this document.\n\n_Last updated: Monday 12 March 2018, 18:00 UTC (log)_\n\n\nIntroduction\n\nThe GStreamer team is proud to announce a new major feature release in\nthe stable 1.x API series of your favourite cross-platform multimedia\nframework!\n\nAs always, this release is again packed with new features, bug fixes and\nother improvements.\n\n\nHighlights\n\n-   WebRTC support: real-time audio/video streaming to and from web\n    browsers\n\n-   Experimental support for the next-gen royalty-free AV1 video codec\n\n-   Video4Linux: encoding support, stable element names and faster\n    device probing\n\n-   Support for the Secure Reliable Transport (SRT) video streaming\n    protocol\n\n-   RTP Forward Error Correction (FEC) support (ULPFEC)\n\n-   RTSP 2.0 support in rtspsrc and gst-rtsp-server\n\n-   ONVIF audio backchannel support in gst-rtsp-server and rtspsrc\n\n-   playbin3 gapless playback and pre-buffering support\n\n-   tee, our stream splitter/duplication element, now does allocation\n    query aggregation which is important for efficient data handling and\n    zero-copy\n\n-   QuickTime muxer has a new prefill recording mode that allows file\n    import in Adobe Premiere and FinalCut Pro while the file is still\n    being written.\n\n-   rtpjitterbuffer fast-start mode and timestamp offset adjustment\n    smoothing\n\n-   souphttpsrc connection sharing, which allows for connection reuse,\n    cookie sharing, etc.\n\n-   nvdec: new plugin for hardware-accelerated video decoding using the\n    NVIDIA NVDEC API\n\n-   Adaptive DASH trick play support\n\n-   ipcpipeline: new plugin that allows splitting a pipeline across\n    multiple processes\n\n-   Major gobject-introspection annotation improvements for large parts\n    of the library API\n\n\nMajor new features and changes\n\nWebRTC support\n\nThere is now basic support for WebRTC in GStreamer in form of a new\nwebrtcbin element and a webrtc support library. This allows you to build\napplications that set up connections with and stream to and from other\nWebRTC peers, whilst leveraging all of the usual GStreamer features such\nas hardware-accelerated encoding and decoding, OpenGL integration,\nzero-copy and embedded platform support. And it's easy to build and\nintegrate into your application too!\n\nWebRTC enables real-time communication of audio, video and data with web\nbrowsers and native apps, and it is supported or about to be support by\nrecent versions of all major browsers and operating systems.\n\nGStreamer's new WebRTC implementation uses libnice for Interactive\nConnectivity Establishment (ICE) to figure out the best way to\ncommunicate with other peers, punch holes into firewalls, and traverse\nNATs.\n\nThe implementation is not complete, but all the basics are there, and\nthe code sticks fairly close to the PeerConnection API. Where\nfunctionality is missing it should be fairly obvious where it needs to\ngo.\n\nFor more details, background and example code, check out Nirbheek's blog\npost _GStreamer has grown a WebRTC implementation_, as well as Matthew's\n_GStreamer WebRTC_ talk from last year's GStreamer Conference in Prague.\n\nNew Elements\n\n-   webrtcbin handles the transport aspects of webrtc connections (see\n    WebRTC section above for more details)\n\n-   New srtsink and srtsrc elements for the Secure Reliable Transport\n    (SRT) video streaming protocol, which aims to be easy to use whilst\n    striking a new balance between reliability and latency for low\n    latency video streaming use cases. More details about SRT and the\n    implementation in GStreamer in Olivier's blog post _SRT in\n    GStreamer_.\n\n-   av1enc and av1dec elements providing experimental support for the\n    next-generation royalty free video AV1 codec, alongside Matroska\n    support for it.\n\n-   hlssink2 is a rewrite of the existing hlssink element, but unlike\n    its predecessor hlssink2 takes elementary streams as input and\n    handles the muxing to MPEG-TS internally. It also leverages\n    splitmuxsink internally to do the splitting. This allows more\n    control over the chunk splitting and sizing process and relies less\n    on the co-operation of an upstream muxer. Different to the old\n    hlssink it also works with pre-encoded streams and does not require\n    close interaction with an upstream encoder element.\n\n-   audiolatency is a new element for measuring audio latency end-to-end\n    and is useful to measure roundtrip latency including both the\n    GStreamer-internal latency as well as latency added by external\n    components or circuits.\n\n-   'fakevideosink is basically a null sink for video data and very\n    similar to fakesink, only that it will answer allocation queries and\n    will advertise support for various video-specific things such\n    GstVideoMeta, GstVideoCropMeta and GstVideoOverlayCompositionMeta\n    like a normal video sink would. This is useful for throughput\n    testing and testing the zero-copy path when creating a new pipeline.\n\n-   ipcpipeline: new plugin that allows the splitting of a pipeline into\n    multiple processes. Usually a GStreamer pipeline runs in a single\n    process and parallelism is achieved by distributing workloads using\n    multiple threads. This means that all elements in the pipeline have\n    access to all the other elements' memory space however, including\n    that of any libraries used. For security reasons one might therefore\n    want to put sensitive parts of a pipeline such as DRM and decryption\n    handling into a separate process to isolate it from the rest of the\n    pipeline. This can now be achieved with the new ipcpipeline plugin.\n    Check out George's blog post _ipcpipeline: Splitting a GStreamer\n    pipeline into multiple processes_ or his lightning talk from last\n    year's GStreamer Conference in Prague for all the gory details.\n\n&nbsp;\n-   proxysink and proxysrc are new elements to pass data from one\n    pipeline to another within the same process, very similar to the\n    existing inter elements, but not limited to raw audio and video\n    data. These new proxy elements are very special in how they work\n    under the hood, which makes them extremely powerful, but also\n    dangerous if not used with care. The reason for this is that it's\n    not just data that's passed from sink to src, but these elements\n    basically establish a two-way wormhole that passes through queries\n    and events in both directions, which means caps negotiation and\n    allocation query driven zero-copy can work through this wormhole.\n    There are scheduling considerations as well: proxysink forwards\n    everything into the proxysrc pipeline directly from the proxysink\n    streaming thread. There is a queue element inside proxysrc to\n    decouple the source thread from the sink thread, but that queue is\n    not unlimited, so it is entirely possible that the proxysink\n    pipeline thread gets stuck in the proxysrc pipeline, e.g. when that\n    pipeline is paused or stops consuming data for some other reason.\n    This means that one should always shut down down the proxysrc\n    pipeline before shutting down the proxysink pipeline, for example.\n    Or at least take care when shutting down pipelines. Usually this is\n    not a problem though, especially not in live pipelines. For more\n    information see Nirbheek's blog post _Decoupling GStreamer\n    Pipelines_, and also check out out the new ipcpipeline plugin for\n    sending data from one process to another process (see above).\n\n-   lcms is a new LCMS-based ICC color profile correction element\n\n-   openmptdec is a new OpenMPT-based decoder for module music formats,\n    such as S3M, MOD, XM, IT. It is built on top of a new\n    GstNonstreamAudioDecoder base class which aims to unify handling of\n    files which do not operate a streaming model. The wildmidi plugin\n    has also been revived and is also implemented on top of this new\n    base class.\n\n-   The curl plugin has gained a new curlhttpsrc element, which is\n    useful for testing HTTP protocol version 2.0 amongst other things.\n\nNoteworthy new API\n\n-   GstPromise provides future/promise-like functionality. This is used\n    in the GStreamer WebRTC implementation.\n\n&nbsp;\n-   GstReferenceTimestampMeta is a new meta that allows you to attach\n    additional reference timestamps to a buffer. These timestamps don't\n    have to relate to the pipeline clock in any way. Examples of this\n    could be an NTP timestamp when the media was captured, a frame\n    counter on the capture side or the (local) UNIX timestamp when the\n    media was captured. The decklink elements make use of this.\n\n&nbsp;\n-   GstVideoRegionOfInterestMeta: it's now possible to attach generic\n    free-form element-specific parameters to a region of interest meta,\n    for example to tell a downstream encoder to use certain codec\n    parameters for a certain region.\n\n&nbsp;\n-   gst_bus_get_pollfd can be used to obtain a file descriptor for the\n    bus that can be poll()-ed on for new messages. This is useful for\n    integration with non-GLib event loops.\n\n&nbsp;\n-   gst_get_main_executable_path() can be used by wrapper plugins that\n    need to find things in the directory where the application\n    executable is located. In the same vein,\n    GST_PLUGIN_DEPENDENCY_FLAG_PATHS_ARE_RELATIVE_TO_EXE can be used to\n    signal that plugin dependency paths are relative to the main\n    executable.\n\n-   pad templates can be told about the GType of the pad subclass of the\n    pad via newly-added GstPadTemplate API API or the\n    gst_element_class_add_static_pad_template_with_gtype() convenience\n    function. gst-inspect-1.0 will use this information to print pad\n    properties.\n\n&nbsp;\n-   new convenience functions to iterate over element pads without using\n    the GstIterator API: gst_element_foreach_pad(),\n    gst_element_foreach_src_pad(), and gst_element_foreach_sink_pad().\n\n&nbsp;\n-   GstBaseSrc and appsrc have gained support for buffer lists:\n    GstBaseSrc subclasses can use gst_base_src_submit_buffer_list(), and\n    applications can use gst_app_src_push_buffer_list() to push a buffer\n    list into appsrc.\n\n&nbsp;\n-   The GstHarness unit test harness has a couple of new convenience\n    functions to retrieve all pending data in the harness in form of a\n    single chunk of memory.\n\n&nbsp;\n-   GstAudioStreamAlign is a new helper object for audio elements that\n    handles discontinuity detection and sample alignment. It will align\n    samples after the previous buffer's samples, but keep track of the\n    divergence between buffer timestamps and sample position (jitter).\n    If it exceeds a configurable threshold the alignment will be reset.\n    This simply factors out code that was duplicated in a number of\n    elements into a common helper API.\n\n&nbsp;\n-   The GstVideoEncoder base class implements Quality of Service (QoS)\n    now. This is disabled by default and must be opted in by setting the\n    \"qos\" property, which will make the base class gather statistics\n    about the real-time performance of the pipeline from downstream\n    elements (usually sinks that sync the pipeline clock). Subclasses\n    can then make use of this by checking whether input frames are late\n    already using gst_video_encoder_get_max_encode_time() If late, they\n    can just drop them and skip encoding in the hope that the pipeline\n    will catch up.\n\n&nbsp;\n-   The GstVideoOverlay interface gained a few helper functions for\n    installing and handling a \"render-rectangle\" property on elements\n    that implement this interface, so that this functionality can also\n    be used from the command line for testing and debugging purposes.\n    The property wasn't added to the interface itself as that would\n    require all implementors to provide it which would not be\n    backwards-compatible.\n\n&nbsp;\n-   A new base class, GstNonstreamAudioDecoder for non-stream audio\n    decoders was added to gst-plugins-bad. This base-class is meant to\n    be used for audio decoders that require the whole stream to be\n    loaded first before decoding can start. Examples of this are module\n    formats (MOD/S3M/XM/IT/etc), C64 SID tunes, video console music\n    files (GYM/VGM/etc), MIDI files and others. The new openmptdec\n    element is based on this.\n\n&nbsp;\n-   Full list of API new in 1.14:\n-   GStreamer core API new in 1.14\n-   GStreamer base library API new in 1.14\n-   gst-plugins-base libraries API new in 1.14\n-   gst-plugins-bad: no list, mostly GstWebRTC library and new\n    non-stream audio decoder base class.\n\nNew RTP features and improvements\n\n-   rtpulpfecenc and rtpulpfecdec are new elements that implement\n    Generic Forward Error Correction (FEC) using Uneven Level Protection\n    (ULP) as described in RFC 5109. This can be used to protect against\n    certain types of (non-bursty) packet loss, and important packets\n    such as those containing codec configuration data or key frames can\n    be protected with higher redundancy. Equally, packets that are not\n    particularly important can be given low priority or not be protected\n    at all. If packets are lost, the receiver can then hopefully restore\n    the lost packet(s) from the surrounding packets which were received.\n    This is an alternative to, or rather complementary to, dealing with\n    packet loss using _retransmission (rtx)_. GStreamer has had\n    retransmission support for a long time, but Forward Error Correction\n    allows for different trade-offs: The advantage of Forward Error\n    Correction is that it doesn't add latency, whereas retransmission\n    requires at least one more roundtrip to request and hopefully\n    receive lost packets; Forward Error Correction increases the\n    required bandwidth however, even in situations where there is no\n    packet loss at all, so one will typically want to fine-tune the\n    overhead and mechanisms used based on the characteristics of the\n    link at the time.\n\n-   New _Redundant Audio Data (RED)_ encoders and decoders for RTP as\n    per RFC 2198 are also provided (rtpredenc and rtpreddec), mostly for\n    chrome webrtc compatibility, as chrome will wrap ULPFEC-protected\n    streams in RED packets, and such streams need to be wrapped and\n    unwrapped in order to use ULPFEC with chrome.\n\n&nbsp;\n-   a few new buffer flags for FEC support:\n    GST_BUFFER_FLAG_NON_DROPPABLE can be used to mark important buffers,\n    e.g. to flag RTP packets carrying keyframes or codec setup data for\n    RTP Forward Error Correction purposes, or to prevent still video\n    frames from being dropped by elements due to QoS. There already is a\n    GST_BUFFER_FLAG_DROPPABLE. GST_RTP_BUFFER_FLAG_REDUNDANT is used to\n    signal internally that a packet represents a redundant RTP packet\n    and used in rtpstorage to hold back the packet and use it only for\n    recovery from packet loss. Further work is still needed in\n    payloaders to make use of these.\n\n-   rtpbin now has an option for increasing timestamp offsets gradually:\n    Instant large changes to the internal ts_offset may cause timestamps\n    to move backwards and also cause visible glitches in media playback.\n    The new \"max-ts-offset-adjustment\" and \"max-ts-offset\" properties\n    let the application control the rate to apply changes to ts_offset.\n    There have also been some EOS/BYE handling improvements in rtpbin.\n\n-   rtpjitterbuffer has a new fast start mode: in many scenarios the\n    jitter buffer will have to wait for the full configured latency\n    before it can start outputting packets. The reason for that is that\n    it often can't know what the sequence number of the first expected\n    RTP packet is, so it can't know whether a packet earlier than the\n    earliest packet received will still arrive in future. This behaviour\n    can now be bypassed by setting the \"faststart-min-packets\" property\n    to the number of consecutive packets needed to start, and the jitter\n    buffer will start output packets as soon as it has N consecutive\n    packets queued internally. This is particularly useful to get a\n    first video frame decoded and rendered as quickly as possible.\n\n-   rtpL8pay and rtpL8depay provide RTP payloading and depayloading for\n    8-bit raw audio\n\nNew element features\n\n-   playbin3 has gained support or gapless playback via the\n    \"about-to-finish\" signal where users can set the uri for the next\n    item to play. For non-live streams this will be emitted as soon as\n    the first uri has finished downloading, so with sufficiently large\n    buffers it is now possible to pre-buffer the next item well ahead of\n    time (unlike playbin where there would not be a lot of time between\n    \"about-to-finish\" emission and the end of the stream). If the stream\n    format of the next stream is the same as that of the previous\n    stream, the data will be concatenated via the concat element.\n    Whether this will result in true gaplessness depends on the\n    container format and codecs used, there might still be codec-related\n    gaps between streams with some codecs.\n\n-   tee now does allocation query aggregation, which is important for\n    zero-copy and efficient data handling, especially for video. Those\n    who want to drop allocation queries on purpose can use the identity\n    element's new \"drop-allocation\" property for that instead.\n\n-   audioconvert now has a \"mix-matrix\" property, which obsoletes the\n    audiomixmatrix element. There's also mix matrix support in the audio\n    conversion and channel mixing API.\n\n-   x264enc: new \"insert-vui\" property to disable VUI (Video Usability\n    Information) parameter insertion into the stream, which allows\n    creation of streams that are compatible with certain legacy hardware\n    decoders that will refuse to decode in certain combinations of\n    resolution and VUI parameters; the max. allowed number of B-frames\n    was also increased from 4 to 16.\n\n-   dvdlpcmdec: has gained support for Blu-Ray audio LPCM.\n\n-   appsrc has gained support for buffer lists (see above) and also seen\n    some other performance improvements.\n\n-   flvmux has been ported to the GstAggregator base class which means\n    it can work in defined-latency mode with live input sources and\n    continue streaming if one of the inputs stops producing data.\n\n-   jpegenc has gained a \"snapshot\" property just like pngenc to make it\n    easier to just output a single encoded frame.\n\n-   jpegdec will now handle interlaced MJPEG streams properly and also\n    handle frames without an End of Image marker better.\n\n-   v4l2: There are now video encoders for VP8, VP9, MPEG4, and H263.\n    The v4l2 video decoder handles dynamic resolution changes, and the\n    video4linux device provider now does much faster device probing. The\n    plugin also no longer uses the libv4l2 library by default, as it has\n    prevented a lot of interesting use cases like CREATE_BUFS, DMABuf,\n    usage of TRY_FMT. As the libv4l2 library is totally inactive and not\n    really maintained, we decided to disable it. This might affect a\n    small number of cheap/old webcams with custom vendor formats for\n    which we do not provide conversion in GStreamer. It is possible to\n    re-enable support for libv4l2 at run-time however, by setting the\n    environment variable GST_V4L2_USE_LIBV4L2=1.\n\n-   rtspsrc now has support for RTSP protocol version 2.0 as well as\n    ONVIF audio backchannels (see below for more details). It also\n    sports a new [\"accept-certificate\"] signal for \"manually\" checking a\n    TLS certificate for validity. It now also prints RTSP/SDP messages\n    to the gstreamer debug log instead of stdout.\n\n-   shout2send now uses non-blocking I/O and has a configurable network\n    operations timeout.\n\n-   splitmuxsink has gained a \"split-now\" action signal and new\n    \"alignment-threshold\" and \"use-robust-muxing\" properties. If robust\n    muxing is enabled, it will check and set the muxer's reserved space\n    properties if present. This is primarily for use with mp4mux's\n    robust muxing mode.\n\n-   qtmux has a new _prefill recording mode_ which sets up a moov header\n    with the correct sample positions beforehand, which then allows\n    software like Adobe Premiere and FinalCut Pro to import the files\n    while they are still being written to. This only works with constant\n    framerate I-frame only streams, and for now only support for ProRes\n    video and raw audio is implemented but adding new codecs is just a\n    matter of defining appropriate maximum frame sizes.\n\n-   qtmux also supports writing of svmi atoms with stereoscopic video\n    information now. Trak timescales can be configured on a per-stream\n    basis using the \"trak-timescale\" property on the sink pads. Various\n    new formats can be muxed: MPEG layer 1 and 2, AC3 and Opus, as well\n    as PNG and VP9.\n\n-   souphttpsrc now does connection sharing by default, shares its\n    SoupSession with other elements in the same pipeline via a\n    GstContext if possible (session-wide settings are all the defaults).\n    This allows for connection reuse, cookie sharing, etc. Applications\n    can also force a context to use. In other news, HTTP headers\n    received from the server are posted as element messages on the bus\n    now for easier diagnostics, and it's also possible now to use other\n    types of proxy servers such as SOCKS4 or SOCKS5 proxies, support for\n    which is implemented directly in gio. Before only HTTP proxies were\n    allowed.\n\n-   qtmux, mp4mux and matroskamux will now refuse caps changes of input\n    streams at runtime. This isn't really supported with these\n    containers (or would have to be implemented differently with a\n    considerable effort) and doesn't produce valid and spec-compliant\n    files that will play everywhere. So if you can't guarantee that the\n    input caps won't change, use a container format that does support on\n    the fly caps changes for a stream such as MPEG-TS or use\n    splitmuxsink which can start a new file when the caps change. What\n    would happen before is that e.g. rtph264depay or rtph265depay would\n    simply send new SPS/PPS inband even for AVC format, which would then\n    get muxed into the container as if nothing changed. Some decoders\n    will handle this just fine, but that's often more luck than by\n    design. In any case, it's not right, so we disallow it now.\n\n-   matroskamux had Table of Content (TOC) support now (chapters etc.)\n    and matroskademux TOC support has been improved. matroskademux has\n    also seen seeking improvements searching for the right cluster and\n    position.\n\n-   videocrop now uses GstVideoCropMeta if downstream supports it, which\n    means cropping can be handled more efficiently without any copying.\n\n-   compositor now has support for _crossfade blending_, which can be\n    used via the new \"crossfade-ratio\" property on the sink pads.\n\n-   The avwait element has a new \"end-timecode\" property and posts\n    \"avwait-status\" element messages now whenever avwait starts or stops\n    passing through data (e.g. because target-timecode and end-timecode\n    respectively have been reached).\n\n&nbsp;\n-   h265parse and h265parse will try harder to make upstream output the\n    same caps as downstream requires or prefers, thus avoiding\n    unnecessary conversion. The parsers also expose chroma format and\n    bit depth in the caps now.\n\n-   The dtls elements now longer rely on or require the application to\n    run a GLib main loop that iterates the default main context\n    (GStreamer plugins should never rely on the application running a\n    GLib main loop).\n\n-   openh264enc allows to change the encoding bitrate dynamically at\n    runtime now\n\n-   nvdec is a new plugin for hardware-accelerated video decoding using\n    the NVIDIA NVDEC API (which replaces the old VDPAU API which is no\n    longer supported by NVIDIA)\n\n-   The NVIDIA NVENC hardware-accelerated video encoders now support\n    dynamic bitrate and preset reconfiguration and support the I420\n    4:2:0 video format. It's also possible to configure the gop size via\n    the new \"gop-size\" property.\n\n-   The MPEG-TS muxer and demuxer (tsmux, tsdemux) now have support for\n    JPEG2000\n\n-   openjpegdec and jpeg2000parse support 2-component images now (gray\n    with alpha), and jpeg2000parse has gained limited support for\n    conversion between JPEG2000 stream-formats. (JP2, J2C, JPC) and also\n    extracts more details such as colorimetry, interlace-mode,\n    field-order, multiview-mode and chroma siting.\n\n-   The decklink plugin for Blackmagic capture and playback cards have\n    seen numerous improvements:\n\n-   decklinkaudiosrc and decklinkvideosrc now put hardware reference\n    timestamp on buffers in form of GstReferenceTimestampMetas.\n    This can be useful to know on multi-channel cards which frames from\n    different channels were captured at the same time.\n\n-   decklinkvideosink has gained support for Decklink hardware keying\n    with two new properties (\"keyer-mode\" and \"keyer-level\") to control\n    the built-in hardware keyer of Decklink cards.\n\n-   decklinkaudiosink has been re-implemented around GstBaseSink instead\n    of the GstAudioBaseSink base class, since the Decklink APIs don't\n    fit very well with the GstAudioBaseSink APIs, which used to cause\n    various problems due to inaccuracies in the clock calculations.\n    Problems were audio drop-outs and A/V sync going wrong after\n    pausing/seeking.\n\n-   support for more than 16 devices, without any artificial limit\n\n-   work continued on the msdk plugin for Intel's Media SDK which\n    enables hardware-accelerated video encoding and decoding on Intel\n    graphics hardware on Windows or Linux. More tuning options were\n    added, and more pixel formats and video codecs are supported now.\n    The encoder now also handles force-key-unit events and can insert\n    frame-packing SEIs for side-by-side and top-bottom stereoscopic 3D\n    video.\n\n-   dashdemux can now do adaptive trick play of certain types of DASH\n    streams, meaning it can do fast-forward/fast-rewind of normal (non-I\n    frame only) streams even at high speeds without saturating network\n    bandwidth or exceeding decoder capabilities. It will keep statistics\n    and skip keyframes or fragments as needed. See Sebastian's blog post\n    _DASH trick-mode playback in GStreamer_ for more details. It also\n    supports webvtt subtitle streams now and has seen improvements when\n    seeking in live streams.\n\n&nbsp;\n-   kmssink has seen lots of fixes and improvements in this cycle,\n    including:\n\n-   Raspberry Pi (vc4) and Xilinx DRM driver support\n\n-   new \"render-rectangle\" property that can be used from the command\n    line as well as \"display-width\" and \"display-height\", and\n    \"can-scale\" properties\n\n-   GstVideoCropMeta support\n\nPlugin and library moves\n\nMPEG-1 audio (mp1, mp2, mp3) decoders and encoders moved to -good\n\nFollowing the expiration of the last remaining mp3 patents in most\njurisdictions, and the termination of the mp3 licensing program, as well\nas the decision by certain distros to officially start shipping full mp3\ndecoding and encoding support, these plugins should now no longer be\nproblematic for most distributors and have therefore been moved from\n-ugly and -bad to gst-plugins-good. Distributors can still disable these\nplugins if desired.\n\nIn particular these are:\n\n-   mpg123audiodec: an mp1/mp2/mp3 audio decoder using libmpg123\n-   lamemp3enc: an mp3 encoder using LAME\n-   twolamemp2enc: an mp2 encoder using TwoLAME\n\nGstAggregator moved from -bad to core\n\nGstAggregator has been moved from gst-plugins-bad to the base library in\nGStreamer and is now stable API.\n\nGstAggregator is a new base class for mixers and muxers that have to\nhandle multiple input pads and aggregate streams into one output stream.\nIt improves upon the existing GstCollectPads API in that it is a proper\nbase class which was also designed with live streaming in mind.\nGstAggregator subclasses will operate in a mode with defined latency if\nany of the inputs are live streams. This ensures that the pipeline won't\nstall if any of the inputs stop producing data, and that the configured\nmaximum latency is never exceeded.\n\nGstAudioAggregator, audiomixer and audiointerleave moved from -bad to -base\n\nGstAudioAggregator is a new base class for raw audio mixers and muxers\nand is based on GstAggregator (see above). It provides defined-latency\nmixing of raw audio inputs and ensures that the pipeline won't stall\neven if one of the input streams stops producing data.\n\nAs part of the move to stabilise the API there were some last-minute API\nchanges and clean-ups, but those should mostly affect internal elements.\n\nIt is used by the audiomixer element, which is a replacement for\n'adder', which did not handle live inputs very well and did not align\ninput streams according to running time. audiomixer should behave much\nbetter in that respect and generally behave as one would expected in\nmost scenarios.\n\nSimilarly, audiointerleave replaces the 'interleave' element which did\nnot handle live inputs or non-aligned inputs very robustly.\n\nGstAudioAggregator and its subclases have gained support for input\nformat conversion, which does not include sample rate conversion though\nas that would add additional latency. Furthermore, GAP events are now\nhandled correctly.\n\nWe hope to move the video equivalents (GstVideoAggregator and\ncompositor) to -base in the next cycle, i.e. for 1.16.\n\nGStreamer OpenGL integration library and plugin moved from -bad to -base\n\nThe GStreamer OpenGL integration library and opengl plugin have moved\nfrom gst-plugins-bad to -base and are now part of the stable API canon.\nNot all OpenGL elements have been moved; a few had to be left behind in\ngst-plugins-bad in the new openglmixers plugin, because they depend on\nthe GstVideoAggregator base class which we were not able to move in this\ncycle. We hope to reunite these elements with the rest of their family\nfor 1.16 though.\n\nThis is quite a milestone, thanks to everyone who worked to make this\nhappen!\n\nQt QML and GTK plugins moved from -bad to -good\n\nThe Qt QML-based qmlgl plugin has moved to -good and provides a\nqmlglsink video sink element as well as a qmlglsrc element. qmlglsink\nrenders video into a QQuickItem, and qmlglsrc captures a window from a\nQML view and feeds it as video into a pipeline for further processing.\nBoth elements leverage GStreamer's OpenGL integration. In addition to\nthe move to -good the following features were added:\n\n-   A proxy object is now used for thread-safe access to the QML widget\n    which prevents crashes in corner case scenarios: QML can destroy the\n    video widget at any time, so without this we might be left with a\n    dangling pointer.\n\n-   EGL is now supported with the X11 backend, which works e.g. on\n    Freescale imx6\n\nThe GTK+ plugin has also moved from -bad to -good. It includes gtksink\nand gtkglsink which both render video into a GtkWidget. gtksink uses\nCairo for rendering the video, which will work everywhere in all\nscenarios but involves an extra memory copy, whereas gtkglsink fully\nleverages GStreamer's OpenGL integration, but might not work properly in\nall scenarios, e.g. where the OpenGL driver does not properly support\nmultiple sharing contexts in different threads; on Linux Nouveau is\nknown to be broken in this respect, whilst NVIDIA's proprietary drivers\nand most other drivers generally work fine, and the experience with\nIntel's driver seems to be fixed; some proprietary embedded Linux\ndrivers don't work; macOS works).\n\nGstPhysMemoryAllocator interface moved from -bad to -base\n\nGstPhysMemoryAllocator is a marker interface for allocators with\nphysical address backed memory.\n\nPlugin removals\n\n-   the sunaudio plugin was removed, since it couldn't ever have been\n    built or used with GStreamer 1.0, but no one even noticed in all\n    these years.\n\n-   the schroedinger-based Dirac encoder/decoder plugin has been\n    removed, as there is no longer any upstream or anyone else\n    maintaining it. Seeing that it's quite a fringe codec it seemed best\n    to simply remove it.\n\nAPI removals\n\n-   some MPEG video parser API in the API unstable codecutils library in\n    gst-plugins-bad was removed after having been deprecated for 5\n    years.\n\n\nMiscellaneous changes\n\n-   The video support library has gained support for a few new pixel\n    formats:\n-   NV16_10LE32: 10-bit variant of NV16, packed into 32bit words (plus 2\n    bits padding)\n-   NV12_10LE32: 10-bit variant of NV12, packed into 32bit words (plus 2\n    bits padding)\n-   GRAY10_LE32: 10-bit grayscale, packed in 32bit words (plus 2 bits\n    padding)\n\n-   decodebin, playbin and GstDiscoverer have seen stability\n    improvements in corner cases such as shutdown while still starting\n    up or shutdown in error cases (hat tip to the oss-fuzz project).\n\n-   floating reference handling was inconsistent and has been cleaned up\n    across the board, including annotations. This solves various\n    long-standing memory leaks in language bindings, which e.g. often\n    caused elements and pads to be leaked.\n\n-   major gobject-introspection annotation improvements for large parts\n    of the library API, including nullability of return types and\n    function parameters, correct types (e.g. strings vs. filenames),\n    ownership transfer, array length parameters, etc. This allows to use\n    bigger parts of the GStreamer API to be safely used from dynamic\n    language bindings (e.g. Python, Javascript) and allows static\n    bindings (e.g. C#, Rust, Vala) to autogenerate more API bindings\n    without manual intervention.\n\nOpenGL integration\n\n-   The GStreamer OpenGL integration library has moved to\n    gst-plugins-base and is now part of our stable API.\n\n-   new MESA3D GBM BACKEND. On devices with working libdrm support, it\n    is possible to use Mesa3D's GBM library to set up an EGL context\n    directly on top of KMS. This makes it possible to use the GStreamer\n    OpenGL elements without a windowing system if a libdrm- and\n    Mesa3D-supported GPU is present.\n\n-   Prefer wayland display over X11: As most Wayland compositors support\n    XWayland, the X11 backend would get selected.\n\n-   gldownload can export dmabufs now, and glupload will advertise\n    dmabuf as caps feature.\n\n\nTracing framework and debugging improvements\n\n-   NEW MEMORY RINGBUFFER BASED DEBUG LOGGER, useful for long-running\n    applications or to retrieve diagnostics when encountering an error.\n    The GStreamer debug logging system provides in-depth debug logging\n    about what is going on inside a pipeline. When enabled, debug logs\n    are usually written into a file, printed to the terminal, or handed\n    off to a log handler installed by the application. However, at\n    higher debug levels the volume of debug output quickly becomes\n    unmanageable, which poses a problem in disk-space or bandwidth\n    restricted environments or with long-running pipelines where a\n    problem might only manifest itself after multiple days. In those\n    situations, developers are usually only interested in the most\n    recent debug log output. The new in-memory ringbuffer logger makes\n    this easy: just installed it with gst_debug_add_ring_buffer_logger()\n    and retrieve logs with gst_debug_ring_buffer_logger_get_logs() when\n    needed. It is possible to limit the memory usage per thread and set\n    a timeout to determine how long messages are kept around. It was\n    always possible to implement this in the application with a custom\n    log handler of course, this just provides this functionality as part\n    of GStreamer.\n\n&nbsp;\n-   'fakevideosink is a null sink for video data that advertises\n    video-specific metas ane behaves like a video sink. See above for\n    more details.\n\n-   gst_util_dump_buffer() prints the content of a buffer to stdout.\n\n-   gst_pad_link_get_name() and gst_state_change_get_name() print pad\n    link return values and state change transition values as strings.\n\n-   The LATENCY TRACER has seen a few improvements: trace records now\n    contain timestamps which is useful to plot things over time, and\n    downstream synchronisation time is now excluded from the measured\n    values.\n\n-   Miniobject refcount tracing and logging was not entirley\n    thread-safe, there were duplicates or missing entries at times. This\n    has now been made reliable.\n\n-   The netsim element, which can be used to simulate network jitter,\n    packet reordering and packet loss, received new features and\n    improvements: it can now also simulate network congestion using a\n    token bucket algorithm. This can be enabled via the \"max-kbps\"\n    property. Packet reordering can be disabled now via the\n    \"allow-reordering\" property: Reordering of packets is not very\n    common in networks, and the delay functions will always introduce\n    reordering if delay > packet-spacing, so by setting\n    \"allow-reordering\" to FALSE you guarantee that the packets are in\n    order, while at the same time introducing delay/jitter to them. By\n    using the new \"delay-distribution\" property the use can control how\n    the delay applied to delayed packets is distributed: This is either\n    the uniform distribution (as before) or the normal distribution; in\n    addition there is also the gamma distribution which simulates the\n    delay on wifi networks better.\n\n\nTools\n\n-   gst-inspect-1.0 now prints pad properties for elements that have pad\n    subclasses with special properties, such as compositor or\n    audiomixer. This only works for elements that use the newly-added\n    GstPadTemplate API API or the\n    gst_element_class_add_static_pad_template_with_gtype() convenience\n    function to tell GStreamer about the special pad subclass.\n\n-   gst-launch-1.0 now generates a gstreamer pipeline diagram (.dot\n    file) whenever SIGHUP is sent to it on Linux/*nix systems.\n\n-   gst-discoverer-1.0 can now analyse live streams such as rtsp:// URIs\n\n\nGStreamer RTSP server\n\n-   Initial support for [RTSP protocol version\n    2.0][rtsp2-lightning-talk] was added, which is to the best of our\n    knowledge the first RTSP 2.0 implementation ever!\n\n-   ONVIF audio backchannel support. This is an extension specified by\n    ONVIF that allows RTSP clients (e.g. a control room operator) to\n    send audio back to the RTSP server (e.g. an IP camera).\n    Theoretically this could have been done also by using the RECORD\n    method of the RTSP protocol, but ONVIF chose not to do that, so the\n    backchannel is set up alongside the other streams. Format\n    negotiation needs to be done out of band, if needed. Use the new\n    ONVIF-specific subclasses GstRTSPOnvifServer and\n    GstRTSPOnvifMediaFactory to enable this functionality.\n\n&nbsp;\n-   The internal server streaming pipeline is now dynamically\n    reconfigured on PLAY based on the transports needed. This means that\n    the server no longer adds the pipeline plumbing for all possible\n    transports from the start, but only if needed as needed. This\n    improves performance and memory footprint.\n\n-   rtspclientsink has gained an \"accept-certificate\" signal for\n    manually checking a TLS certificate for validity.\n\n-   Fix keep-alive/timeout issue for certain clients using TCP\n    interleave as transport who don't do keep-alive via some other\n    method such as periodic RTSP OPTION requests. We now put netaddress\n    metas on the packets from the TCP interleaved stream, so can map\n    RTCP packets to the right stream in the server and can handle them\n    properly.\n\n-   Language bindings improvements: in general there were quite a few\n    improvements in the gobject-introspection annotations, but we also\n    extended the permissions API which was not usable from bindings\n    before.\n\n-   Fix corner case issue where the wrong mount point was found when\n    there were multiple mount points with a common prefix.\n\n\nGStreamer VAAPI\n\n-   this section will be filled in shortly {FIXME!}\n\n\nGStreamer Editing Services and NLE\n\n-   this section will be filled in shortly {FIXME!}\n\n\nGStreamer validate\n\n-   this section will be filled in shortly {FIXME!}\n\n\nGStreamer Python Bindings\n\n-   this section will be filled in shortly {FIXME!}\n\n\nBuild and Dependencies\n\n-   the new WebRTC support in gst-plugins-bad depends on the GStreamer\n    elements that ship as part of libnice, and libnice version 1.1.14 is\n    required. Also the dtls and srtp plugins.\n\n-   gst-plugins-bad no longer depends on the libschroedinger Dirac codec\n    library.\n\n-   The srtp plugin can now also be built against libsrtp2.\n\n-   some plugins and libraries have moved between modules, see the\n    _Plugin and_ _library moves_ section above, and their respective\n    dependencies have moved with them of course, e.g. the GStreamer\n    OpenGL integration support library and plugin is now in\n    gst-plugins-base, and mpg123, LAME and twoLAME based audio decoder\n    and encoder plugins are now in gst-plugins-good.\n\n-   Unify static and dynamic plugin interface and remove plugin specific\n    static build option: Static and dynamic plugins now have the same\n    interface. The standard --enable-static/--enable-shared toggle is\n    sufficient. This allows building static and shared plugins from the\n    same object files, instead of having to build everything twice.\n\n-   The default plugin entry point has changed. This will only affect\n    plugins that are recompiled against new GStreamer headers. Binary\n    plugins using the old entry point will continue to work. However,\n    plugins that are recompiled must have matching plugin names in\n    GST_PLUGIN_DEFINE and filenames, as the plugin entry point for\n    shared plugins is now deduced from the plugin filename. This means\n    you can no longer have a plugin called foo living in a file called\n    libfoobar.so or such, the plugin filename needs to match. This might\n    cause problems with some external third party plugin modules when\n    they get rebuilt against GStreamer 1.14.\n\n\nNote to packagers and distributors\n\nA number of libraries, APIs and plugins moved between modules and/or\nlibraries in different modules between version 1.12.x and 1.14.x, see\nthe _Plugin and_ _library moves_ section above. Some APIs have seen\nminor ABI changes in the course of moving them into the stable APIs\nsection.\n\nThis means that you should try to ensure that all major GStreamer\nmodules are synced to the same major version (1.12 or 1.13/1.14) and can\nonly be upgraded in lockstep, so that your users never end up with a mix\nof major versions on their system at the same time, as this may cause\nbreakages.\n\nAlso, plugins compiled against >= 1.14 headers will not load with\nGStreamer <= 1.12 owing to a new plugin entry point (but plugin binaries\nbuilt against older GStreamer versions will continue to load with newer\nversions of GStreamer of course).\n\nThere is also a small structure size related ABI breakage introduced in\nthe gst-plugins-bad codecparsers library between version 1.13.90 and\n1.13.91. This should \"only\" affect gstreamer-vaapi, so anyone who ships\nthe release candidates is advised to upgrade those two modules at the\nsame time.\n\n\nPlatform-specific improvements\n\nAndroid\n\n-   ahcsrc (Android camera source) does autofocus now\n\nmacOS and iOS\n\n-   this section will be filled in shortly {FIXME!}\n\nWindows\n\n-   The GStreamer wasapi plugin was rewritten and should not only be\n    usable now, but in top shape and suitable for low-latency use cases.\n    The Windows Audio Session API (WASAPI) is Microsoft's most modern\n    method for talking with audio devices, and now that the wasapi\n    plugin is up to scratch it is preferred over the directsound plugin.\n    The ranks of the wasapisink and wasapisrc elements have been updated\n    to reflect this. Further improvements include:\n\n-   support for more than 2 channels\n\n-   a new \"low-latency\" property to enable low-latency operation (which\n    should always be safe to enable)\n\n-   support for the AudioClient3 API which is only available on Windows\n    10: in wasapisink this will be used automatically if available; in\n    wasapisrc it will have to be enabled explicitly via the\n    \"use-audioclient3\" property, as capturing audio with low latency and\n    without glitches seems to require setting the realtime priority of\n    the entire pipeline to \"critical\", which cannot be done from inside\n    the element, but has to be done in the application.\n\n-   set realtime thread priority to avoid glitches\n\n-   allow opening devices in exclusive mode, which provides much lower\n    latency compared to shared mode where WASAPI's engine period is\n    10ms. This can be activated via the \"exclusive\" property.\n\n-   There are now GstDeviceProvider implementations for the wasapi and\n    directsound plugins, so it's now possible to discover both audio\n    sources and audio sinks on Windows via the GstDeviceMonitor API\n\n-   debug log timestamps are now higher granularity owing to\n    g_get_monotonic_time() now being used as fallback in\n    gst_utils_get_timestamp(). Before that, there would sometimes be\n    10-20 lines of debug log output sporting the same timestamp.\n\n\nContributors\n\nAaron Boxer, Adrián Pardini, Adrien SCH, Akinobu Mita, Alban Bedel,\nAlessandro Decina, Alex Ashley, Alicia Boya García, Alistair Buxton,\nAlvaro Margulis, Anders Jonsson, Andreas Frisch, Andrejs Vasiljevs,\nAndrew Bott, Antoine Jacoutot, Antonio Ospite, Antoni Silvestre, Anton\nObzhirov, Anuj Jaiswal, Arjen Veenhuizen, Arnaud Bonatti, Arun Raghavan,\nAshish Kumar, Aurélien Zanelli, Ayaka, Branislav Katreniak, Branko\nSubasic, Brion Vibber, Carlos Rafael Giani, Cassandra Rommel, Chris\nBass, Chris Paulson-Ellis, Christoph Reiter, Claudio Saavedra, Clemens\nLang, Cyril Lashkevich, Daniel van Vugt, Dave Craig, Dave Johnstone,\nDavid Evans, David Schleef, Deepak Srivastava, Dimitrios Katsaros,\nDmitry Zhadinets, Dongil Park, Dustin Spicuzza, Eduard Sinelnikov,\nEdward Hervey, Enrico Jorns, Eunhae Choi, Ezequiel Garcia, fengalin,\nFilippo Argiolas, Florent Thiéry, Florian Zwoch, Francisco Velazquez,\nFrançois Laignel, fvanzile, George Kiagiadakis, Georg Lippitsch, Graham\nLeggett, Guillaume Desmottes, Gurkirpal Singh, Gwang Yoon Hwang, Gwenole\nBeauchesne, Haakon Sporsheim, Haihua Hu, Håvard Graff, Heekyoung Seo,\nHeinrich Fink, Holger Kaelberer, Hoonhee Lee, Hosang Lee, Hyunjun Ko,\nIan Jamison, James Stevenson, Jan Alexander Steffens (heftig), Jan\nSchmidt, Jason Lin, Jens Georg, Jeremy Hiatt, Jérôme Laheurte, Jimmy\nOhn, Jochen Henneberg, John Ludwig, John Nikolaides, Jonathan Karlsson,\nJosep Torra, Juan Navarro, Juan Pablo Ugarte, Julien Isorce, Jun Xie,\nJussi Kukkonen, Justin Kim, Lasse Laursen, Lubosz Sarnecki, Luc\nDeschenaux, Luis de Bethencourt, Marcin Lewandowski, Mario Alfredo\nCarrillo Arevalo, Mark Nauwelaerts, Martin Kelly, Matej Knopp, Mathieu\nDuponchelle, Matteo Valdina, Matt Fischer, Matthew Waters, Matthieu\nBouron, Matthieu Crapet, Matt Staples, Michael Catanzaro, Michael\nOlbrich, Michael Shigorin, Michael Tretter, Michał Dębski, Michał Górny,\nMichele Dionisio, Miguel París, Mikhail Fludkov, Munez, Nael Ouedraogo,\nNeos3452, Nicholas Panayis, Nick Kallen, Nicola Murino, Nicolas\nDechesne, Nicolas Dufresne, Nirbheek Chauhan, Ognyan Tonchev, Ole André\nVadla Ravnås, Oleksij Rempel, Olivier Crête, Omar Akkila, Orestis\nFloros, Patricia Muscalu, Patrick Radizi, Paul Kim, Per-Erik Brodin,\nPeter Seiderer, Philip Craig, Philippe Normand, Philippe Renon, Philipp\nZabel, Pierre Pouzol, Piotr Drąg, Ponnam Srinivas, Pratheesh Gangadhar,\nRaimo Järvi, Ramprakash Jelari, Ravi Kiran K N, Reynaldo H. Verdejo\nPinochet, Rico Tzschichholz, Robert Rosengren, Roland Peffer, Руслан\nИжбулатов, Sam Hurst, Sam Thursfield, Sangkyu Park, Sanjay NM, Satya\nPrakash Gupta, Scott D Phillips, Sean DuBois, Sebastian Cote, Sebastian\nDröge, Sebastian Rasmussen, Sejun Park, Sergey Borovkov, Seungha Yang,\nShakin Chou, Shinya Saito, Simon Himmelbauer, Sky Juan, Song Bing,\nSreerenj Balachandran, Stefan Kost, Stefan Popa, Stefan Sauer, Stian\nSelnes, Thiago Santos, Thibault Saunier, Thijs Vermeir, Tim Allen,\nTim-Philipp Müller, Ting-Wei Lan, Tomas Rataj, Tom Bailey, Tonu Jaansoo,\nU. Artie Eoff, Umang Jain, Ursula Maplehurst, VaL Doroshchuk, Vasilis\nLiaskovitis, Víctor Manuel Jáquez Leal, vijay, Vincent Penquerc'h,\nVineeth T M, Vivia Nikolaidou, Wang Xin-yu (王昕宇), Wei Feng, Wim\nTaymans, Wonchul Lee, Xabier Rodriguez Calvar, Xavier Claessens,\nXuGuangxin, Yasushi SHOJI, Yi A Wang, Youness Alaoui,\n\n... and many others who have contributed bug reports, translations, sent\nsuggestions or helped testing.\n\n\nBugs fixed in 1.14\n\nMore than 800 bugs have been fixed during the development of 1.14.\n\nThis list does not include issues that have been cherry-picked into the\nstable 1.12 branch and fixed there as well, all fixes that ended up in\nthe 1.12 branch are also included in 1.14.\n\nThis list also does not include issues that have been fixed without a\nbug report in bugzilla, so the actual number of fixes is much higher.\n\n\nStable 1.14 branch\n\nAfter the 1.14.0 release there will be several 1.14.x bug-fix releases\nwhich will contain bug fixes which have been deemed suitable for a\nstable branch, but no new features or intrusive changes will be added to\na bug-fix release usually. The 1.14.x bug-fix releases will be made from\nthe git 1.14 branch, which is a stable branch.\n\n1.14.0\n\n1.14.0 is scheduled to be released in early March 2018.\n\n\nKnown Issues\n\n-   The webrtcdsp element (which is unrelated to the newly-landed\n    GStreamer webrtc support) is currently not shipped as part of the\n    Windows binary packages due to a build system issue.\n\n\nSchedule for 1.16\n\nOur next major feature release will be 1.16, and 1.15 will be the\nunstable development version leading up to the stable 1.16 release. The\ndevelopment of 1.15/1.16 will happen in the git master branch.\n\nThe plan for the 1.16 development cycle is yet to be confirmed, but it\nis expected that feature freeze will be around August 2017 followed by\nseveral 1.15 pre-releases and the new 1.16 stable release in September.\n\n1.16 will be backwards-compatible to the stable 1.14, 1.12, 1.10, 1.8,\n1.6, 1.4, 1.2 and 1.0 release series.\n\n------------------------------------------------------------------------\n\n_These release notes have been prepared by Tim-Philipp Müller with_\n_contributions from Sebastian Dröge._\n\n_License: CC BY-SA 4.0_",
  "requireLicenseAcceptance": false,
  "summary": "GstSharp c# bindings",
  "verbatimVersion": "1.13.90.1",
  "version": "1.13.90.1",
  "packageEntries": [
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#GstSharp.nuspec",
      "@type": "PackageEntry",
      "compressedLength": 21110,
      "fullName": "GstSharp.nuspec",
      "length": 53725,
      "name": "GstSharp.nuspec"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#lib/net45/gstreamer-sharp.dll",
      "@type": "PackageEntry",
      "compressedLength": 420638,
      "fullName": "lib/net45/gstreamer-sharp.dll",
      "length": 1497088,
      "name": "gstreamer-sharp.dll"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#build/net45/gstreamer-sharp.dll.config",
      "@type": "PackageEntry",
      "compressedLength": 313,
      "fullName": "build/net45/gstreamer-sharp.dll.config",
      "length": 2598,
      "name": "gstreamer-sharp.dll.config"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#lib/net45/gio-sharp.dll",
      "@type": "PackageEntry",
      "compressedLength": 199560,
      "fullName": "lib/net45/gio-sharp.dll",
      "length": 728064,
      "name": "gio-sharp.dll"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#build/net45/gio-sharp.dll.config",
      "@type": "PackageEntry",
      "compressedLength": 90,
      "fullName": "build/net45/gio-sharp.dll.config",
      "length": 223,
      "name": "gio-sharp.dll.config"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#lib/net45/glib-sharp.dll",
      "@type": "PackageEntry",
      "compressedLength": 65693,
      "fullName": "lib/net45/glib-sharp.dll",
      "length": 164864,
      "name": "glib-sharp.dll"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#build/net45/glib-sharp.dll.config",
      "@type": "PackageEntry",
      "compressedLength": 82,
      "fullName": "build/net45/glib-sharp.dll.config",
      "length": 163,
      "name": "glib-sharp.dll.config"
    },
    {
      "@id": "https://api.nuget.org/v3/catalog0/data/2018.03.14.11.47.22/gstsharp.1.13.90.1.json#build/GstSharp.targets",
      "@type": "PackageEntry",
      "compressedLength": 324,
      "fullName": "build/GstSharp.targets",
      "length": 496,
      "name": "GstSharp.targets"
    }
  ],
  "tags": [
    "GStreamer",
    "GstSharp",
    "bindings"
  ],
  "@context": {
    "@vocab": "http://schema.nuget.org/schema#",
    "catalog": "http://schema.nuget.org/catalog#",
    "xsd": "http://www.w3.org/2001/XMLSchema#",
    "dependencies": {
      "@id": "dependency",
      "@container": "@set"
    },
    "dependencyGroups": {
      "@id": "dependencyGroup",
      "@container": "@set"
    },
    "packageEntries": {
      "@id": "packageEntry",
      "@container": "@set"
    },
    "supportedFrameworks": {
      "@id": "supportedFramework",
      "@container": "@set"
    },
    "tags": {
      "@id": "tag",
      "@container": "@set"
    },
    "published": {
      "@type": "xsd:dateTime"
    },
    "created": {
      "@type": "xsd:dateTime"
    },
    "lastEdited": {
      "@type": "xsd:dateTime"
    },
    "catalog:commitTimeStamp": {
      "@type": "xsd:dateTime"
    }
  }
}